---
title: 'تثبيت وتشغيل نماذج الذكاء الإصطناعي (LLMs) على لينكس'
excerpt: 'تثبيت وتشغيل نماذج الذكاء الإصطناعي (LLMs) على لينكس'
status: published
publishedDate: 2025-04-10
tags:
  - لينكس
  - ai
keywords:
  - AI
  - ollama
  - لينكس
  - ذكاء اصطناعي
  - LLM
  - LLMs
---
يُعد **Ollama** مشروعًا مفتوح المصدر مصممًا لتبسيط عملية نشر وإدارة **نماذج اللغة الكبيرة (Large Language Models - LLMs)** على الأنظمة الشخصية أو ضمن الشبكات الخاصة. يتمثل هدفه الأساسي في تمكين المطورين والباحثين والمؤسسات من الاستفادة من قدرات نماذج اللغة الكبيرة المتطورة باستخدام الموارد المحلية. هذا النهج يلغي الحاجة إلى البنية التحتية السحابية الخارجية، مما يضمن بقاء البيانات داخل أنظمة المستخدمين وعدم مغادرتها أبدًا.

ما يميز Ollama عن منصات نماذج اللغة الكبيرة الأخرى هو التزامه بالبساطة و**الخصوصية (privacy)**. من خلال تشغيل نماذج اللغة الكبيرة محليًا، يكتسب المستخدمون ملكية كاملة لبياناتهم ويتمتعون بالمرونة في تخصيص نماذج الذكاء الاصطناعي لتلبية احتياجاتهم الفريدة. يوفر هذا النهج الذي يركز على **التشغيل المحلي (local deployment)** تحكمًا لا مثيل له في البيانات الحساسة، وهو جانب حيوي للمؤسسات التي تسعى لتجنب المخاطر المرتبطة بإرسال البيانات إلى خوادم خارجية. سواء كان الأمر يتعلق بمشروع شخصي أو تطبيق مؤسسي، فإن Ollama يمكّن المستخدمين من الحفاظ على إشراف كامل.

إن قيمة Ollama تكمن في قدرته على توفير الخصوصية، والتحكم، والتنفيذ المحلي، مما يجعله حلاً مثاليًا لاحتياجات الذكاء الاصطناعي المحلية. إنه ذو قيمة خاصة للصناعات التي تتطلب خصوصية بيانات قصوى، مثل الرعاية الصحية والقطاع المالي والحكومي، نظرًا لقدرته على العمل دون اتصال بالإنترنت أو في بيئات آمنة.

---

## 2. ميزات Ollama الرئيسية ومزاياه

يقدم Ollama مجموعة من الميزات التي تمنحه ميزة واضحة في مجال نماذج اللغة الكبيرة المحلية:

### النشر المحلي والتحكم في البيانات
إن الميزة الأبرز لـ Ollama هي قدرته على نشر نماذج اللغة الكبيرة محليًا، مما يضمن أن جميع عمليات معالجة البيانات تتم داخل بيئة المستخدم. يمنح هذا النهج الذي يركز على التشغيل المحلي تحكمًا لا مثيل له في البيانات الحساسة، وهو أمر بالغ الأهمية لتجنب المخاطر المرتبطة بإرسال البيانات إلى خوادم خارجية. هذه القدرة ضرورية للمشاريع الشخصية والتطبيقات المؤسسية على حد سواء.

### لا حاجة لموارد السحابة وكفاءة التكلفة
أحد العوائق الرئيسية أمام تبني حلول الذكاء الاصطناعي هو الاعتماد على الخدمات السحابية، والتي يمكن أن تكون مكلفة وتطرح مخاطر أمنية محتملة. يلغي Ollama هذا الاعتماد من خلال الاستفادة من الأجهزة المحلية، مما يجعل تطوير الذكاء الاصطناعي متاحًا حتى لأولئك الذين لديهم قيود في الميزانية أو سياسات أمنية صارمة. هذه الميزة تقلل أيضًا من مشكلات **زمن الوصول (latency)**، مما يسمح بالتفاعلات في الوقت الفعلي دون تكلفة الاتصال السحابي.

### المرونة والتخصيص وقابلية التوسع
تعد المرونة جوهر تصميم Ollama. يمكن للمستخدمين ضبط النماذج لتناسب حالات الاستخدام المحددة لديهم، سواء كانت معالجة اللغة، أو أتمتة خدمة العملاء، أو التوصيات المخصصة. تتيح المنصة التكامل مع الأدوات والأنظمة الحالية، مما يسهل تحسين سير العمل دون إعادة هندسة التطبيقات بأكملها.

---

## 3. تحضير نظام Linux الخاص بك لـ Ollama

قبل الشروع في تثبيت Ollama، من الضروري التأكد من أن نظام **Linux (لينكس)** الخاص بك يلبي المتطلبات المسبقة اللازمة. هذه المتطلبات تضمن التشغيل السلس والفعال لـ Ollama والنماذج التي سيتم تشغيلها.

### متطلبات النظام الأساسية
- **نظام التشغيل**: Ollama متوافق مع توزيعات Linux الرئيسية مثل **Ubuntu (أوبونتو)** (يوصى بالإصدار 22.04 أو أحدث)، **Debian (دبيان)**، **CentOS**، و**Fedora (فيدورا)**.
- **المعالج**: يتطلب معالجًا بمعمارية 64-بت (x86-64).
- **ذاكرة الوصول العشوائي (RAM)**: الحد الأدنى 8 جيجابايت، ولكن يوصى بشدة بـ 16 جيجابايت لنماذج الذكاء الاصطناعي المعقدة ولأداء أفضل.
- **مساحة التخزين**: مطلوب 10 جيجابايت على الأقل من مساحة القرص الحرة، ولكن نماذج اللغة الكبيرة يمكن أن تستهلك مساحة تخزين كبيرة، لذا يفضل توفير المزيد.
- **وحدة معالجة الرسوميات (GPU)**: اختيارية، ولكن يوصى بها بشدة لتحسين الأداء. يدعم Ollama وحدات معالجة الرسوميات من **NVIDIA** و **AMD**.
- **الوصول**: مطلوب وصول **الجذر (root)** أو حساب بامتيازات `sudo` للتثبيت.

### فحوصات ما قبل التثبيت
1.  **التحقق من توافق النظام**: تأكد من أن نظامك 64-بت باستخدام الأمر `uname -m` في الطرفية.
2.  **تحديث حزم النظام**: قم بتحديث قائمة الحزم وترقية الحزم المثبتة باستخدام الأمر `sudo apt update && sudo apt upgrade`.
3.  **تثبيت التبعيات المطلوبة**: يتطلب Ollama **Python**، **Pip**، و **Git**. قم بتثبيتها باستخدام الأمر `sudo apt install python3 python3-pip git`، ثم تحقق من التثبيت باستخدام `python3 --version` و `pip3 --version` و `git --version`.

---

## 4. خطوة بخطوة: تثبيت Ollama على Linux

يتميز Ollama بتقديم طريقة تثبيت مباشرة لنظام Linux عبر أمر واحد.

### أمر التثبيت البسيط
افتح الطرفية وقم بتنفيذ الأمر التالي: `curl -fsSL https://ollama.com/install.sh | sh`.

### التحقق من التثبيت
بعد اكتمال التثبيت، تأكد من أن Ollama يعمل بنجاح عن طريق التحقق من إصداره عبر الأمر: `ollama --version`.

### إدارة خدمة Ollama
- **التحقق من حالة الخدمة**: للتحقق مما إذا كانت خدمة Ollama تعمل، استخدم `sudo systemctl status ollama`.
- **التمكين عند الإقلاع**: لتكوين Ollama ليبدأ تلقائيًا عند إقلاع نظامك، قم بتنفيذ `sudo systemctl enable ollama`.
- **إعادة تشغيل الخدمة**: إذا لزم الأمر، أعد تشغيل خدمة Ollama باستخدام `sudo systemctl restart ollama`.

---

## 5. نموذج اللغة الكبير المحلي الأول الخاص بك: تشغيل النماذج باستخدام Ollama

يأتي Ollama مع مستودع نماذج مدمج، مما يسهل العثور على نماذج اللغة الكبيرة وتنزيلها وتشغيلها محليًا.

### استكشاف واختيار النماذج
- يمكن استكشاف النماذج المتاحة على موقع **مكتبة Ollama الرسمي** (`ollama.com/library`).
- تختلف النماذج في الحجم (مثل 3B, 8B, 70B)، والقدرات (نص، صور، أدوات). تشمل الأمثلة **Gemma** (نماذج جوجل خفيفة الوزن)، **Llama 3** (نماذج ميتا الأساسية)، و **Mistral**.
- يجب مراعاة النماذج **الكمّية (Quantized)** (مثل `q4_0`) التي تم تحسينها لتقليل استهلاك الذاكرة مع الحفاظ على الجودة، وهي مثالية للبيئات محدودة الموارد.

### تنزيل النماذج (`ollama pull`)
لتنزيل نموذج دون تشغيله فورًا، استخدم الأمر `ollama pull MODEL_NAME`، على سبيل المثال: `ollama pull llama3`.
للتحقق من جميع النماذج المثبتة حاليًا على جهازك، استخدم `ollama list`.

### التفاعل مع النماذج (`ollama run`)
لتنزيل نموذج وبدء استخدامه على الفور في الطرفية، قم بتشغيل الأمر `ollama run MODEL_NAME`، على سبيل المثال: `ollama run llama3`.
يطلق هذا الأمر واجهة محادثة تفاعلية. للخروج من النموذج، اكتب `/bye` أو استخدم `Ctrl+D`.

### جدول الأوامر الأساسية لـ Ollama في واجهة سطر الأوامر (CLI)

{% table %}
---
- **الأمر**
- **الغرض**
- **مثال**
---
- `ollama run`
- سحب وبدء تشغيل نموذج تفاعلي
- `ollama run llama3`
---
- `ollama pull`
- تنزيل نموذج دون تشغيله
- `ollama pull mistral`
---
- `ollama list`
- عرض جميع النماذج المحلية
- `ollama list`
---
- `ollama rm`
- إزالة نموذج لتحرير مساحة القرص
- `ollama rm mistral`
---
- `ollama show`
- فحص بيانات تعريف النموذج
- `ollama show llama3`
---
- `ollama ps`
- عرض النماذج التي تعمل حاليًا
- `ollama ps`
---
- `ollama stop`
- إيقاف نموذج يعمل في الخلفية
- `ollama stop llama3`
---
- `ollama create`
- إنشاء نموذج مخصص من ملف Modelfile
- `ollama create mymodel -f ./Modelfile`
---
- `ollama help`
- عرض المساعدة حول أي أمر
- `ollama help pull`
{% /table %}

---

## 6. تحسين تجربة Ollama واستكشاف الأخطاء وإصلاحها

لضمان أفضل أداء واستقرار، من المهم اتباع أفضل الممارسات في اختيار النماذج وإدارة الموارد.

### أفضل الممارسات
- **مراقبة الموارد**: راقب باستمرار استخدام **وحدة المعالجة المركزية (CPU)**، و**ذاكرة الوصول العشوائي (RAM)**، و**وحدة معالجة الرسوميات (GPU)**.
- **اختيار حجم النموذج المناسب**: اختر النماذج التي تتوافق مع قدرات أجهزتك. تتطلب النماذج الأكبر المزيد من الذاكرة.
- **النماذج الكمّية (Quantized)**: أعطِ الأولوية للنماذج الكمّية (مثل `q4_0`) للبيئات محدودة الموارد.
- **تسريع وحدة معالجة الرسوميات (GPU)**: استخدم تسريع GPU كلما أمكن ذلك للحصول على أداء أفضل بكثير.
- **التنظيف الدوري**: راجع النماذج المخزنة بشكل دوري باستخدام `ollama list` وقم بإزالة النماذج غير المستخدمة باستخدام `ollama rm`.

### جدول استكشاف الأخطاء الشائعة وإصلاحها

{% table %}
---
- **العرض**
- **السبب المحتمل**
- **الحل / الأمر المقترح**
---
- **فشل تنزيل النماذج**
- عدم كفاية مساحة القرص
- تحقق من المساحة (`df -h`) وحجم مجلد النماذج (`du -sh ~/.ollama/models`). أزل النماذج غير المستخدمة.
---
- **Ollama لا يستجيب**
- تعارض داخلي أو إعدادات غير صحيحة
- أعد تشغيل الخدمة: `sudo systemctl restart ollama`.
---
- **وحدة GPU غير مكتشفة**
- مشاكل في **برامج التشغيل (Drivers)**
- **NVIDIA**: تأكد من تحديث برامج التشغيل. أعد تشغيل الجهاز. **AMD**: تحقق من سجلات `dmesg` بحثًا عن أخطاء `amdgpu`.
---
- **تعارض المنفذ (Port 11434)**
- المنفذ الافتراضي قيد الاستخدام
- تحقق من المنفذ (`sudo lsof -i :11434`) واستخدم منفذًا بديلاً: `export OLLAMA_HOST=127.0.0.1:11435`.
---
- **بطء أو تعطل النماذج**
- ذاكرة غير كافية
- اختر نموذجًا أصغر حجمًا أو نسخة كمّية.
---
- **الأمر `ollama` غير موجود**
- مسار `ollama` غير مضاف إلى متغير `PATH`
- أضف المسار إلى ملف التكوين (مثل `.bashrc`): `export PATH=$PATH:/usr/local/bin`.
---
- **فشل تحميل النموذج**
- تلف النموذج أو دليل غير صحيح
- أعد تنزيل النموذج: `ollama pull model_name`. راجع سجلات Ollama: `journalctl -u ollama`.
{% /table %}

---

## 7. الخلاصة

يمثل **Ollama** حلاً قويًا ومرنًا لنشر وإدارة **نماذج اللغة الكبيرة (LLMs)** محليًا على أنظمة **Linux**. من خلال إعطاء الأولوية للخصوصية، والتحكم في البيانات، والاستقلال عن الحلول السحابية، فإنه يمكّن المستخدمين من الاستفادة من الذكاء الاصطناعي المتطور دون المساومة على الأمان أو التكلفة.

لتحقيق أقصى استفادة من تجربة Ollama، يوصى بما يلي:
- **مطابقة الأجهزة مع النماذج**: قم دائمًا بتقييم موارد نظامك (خاصة RAM و GPU) قبل اختيار النماذج.
- **الإدارة الاستباقية للموارد**: راقب استخدام القرص بانتظام، واحذف النماذج غير المستخدمة.
- **إتقان أساسيات واجهة سطر الأوامر (CLI)**: فهم الأوامر الأساسية مثل `ollama run` و `ollama list` سيحسن بشكل كبير من كفاءة سير عملك.
- **الاستعداد لاستكشاف الأخطاء وإصلاحها**: معرفة كيفية التحقق من حالة الخدمة ومراجعة السجلات ستمكنك من حل المشكلات الشائعة.
- **البقاء على اطلاع**: حافظ على تحديث Ollama وبرامج تشغيل نظامك (Drivers) بانتظام.

من خلال تبني هذه الممارسات، يمكن للمستخدمين تسخير الإمكانات الكاملة لـ **Ollama**، وتحويل أنظمة **Linux** الخاصة بهم إلى مراكز قوية لتطوير الذكاء الاصطناعي المحلي.